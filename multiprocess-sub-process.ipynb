{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Commonly Used Words by Each Author - Parallelize Approach - One Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start time calculating\n",
    "start= timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiprocess funaction for dataframe processing\n",
    "from multiprocess import Pool, TimeoutError, cpu_count\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### OPERATION 1 ###\n",
    "\n",
    "#LOAD data\n",
    "df = pd.read_csv(\"D:/UOB Master BDSA/BDSA606 High Performance Computing/Project/quotes.csv\")\n",
    "#df = pd.read_csv('/dbfs/FileStore/tables/quotes.csv')\n",
    "\n",
    "#DROP column category as not required for this problem\n",
    "df=df.drop(columns=['category'])\n",
    "\n",
    "#view sample data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### OPERATION 2 ####\n",
    "\n",
    "print(\"Dataset shape is:\",df.shape) #view shape of dataset\n",
    "print(\"\\nThere are missing values under columns:\\n\",df.isnull().sum()) #check missing values\n",
    "\n",
    "df=df.dropna() #drop rows with missing values\n",
    "print(\"\\nMissing values dropped, now there are no missing values:\\n\",df.isnull().sum()) #check missing values\n",
    "print(\"\\nDataset shape after dropping missing values:\",df.shape) #reiew dataset shape\n",
    "\n",
    "#JOIN quotes for each author\n",
    "df2=df.groupby(['author'])['quote'].apply(','.join).reset_index()\n",
    "print(\"\\nDataset shape after joining quotes for each author:\",df2.shape) #reiew dataset shape\n",
    "print(\"\\nThere are missing values after joining quotes:\\n\",df2.isnull().sum()) #check missing values\n",
    "df2.head(3) #view combined quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### OPERATION 3 ####\n",
    "\n",
    "#CLEAN text of quotes\n",
    "df2.replace(\"'\",'\"',inplace=True)\n",
    "df2.quote = df2.quote.str.replace(' ', '.')\n",
    "df2.quote = df2.quote.str.replace(',', '.')\n",
    "df2.quote = df2.quote.str.replace('-', '.')\n",
    "df2.quote = df2.quote.str.replace('_', '.')\n",
    "df2.quote = df2.quote.str.replace('#', '.')\n",
    "df2.quote = df2.quote.str.replace('$', '.')\n",
    "df2.quote = df2.quote.str.replace('/', '.')\n",
    "df2.quote = df2.quote.str.replace('\\'', '.')\n",
    "df2.quote = df2.quote.str.replace('&', '.')\n",
    "df2.quote = df2.quote.str.replace('\"', '.')\n",
    "df2.quote = df2.quote.str.replace('@', '.')\n",
    "df2.quote = df2.quote.str.replace(':', '.')\n",
    "df2.quote = df2.quote.str.replace('â€™', '.')\n",
    "df2.head(10) #view updates values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### OPERATION 4 ####\n",
    "\n",
    "#SPLIT quote into words \n",
    "from itertools import chain\n",
    "cols = df2.columns.difference(['quote'])\n",
    "words = df2['quote'].str.split('.')\n",
    "df3 =  (df2.loc[df2.index.repeat(words.str.len()), cols]\n",
    "         .assign(words=list(chain.from_iterable(words.tolist()))))\n",
    "\n",
    "print(\"Dataset shape after splitting quotes into words is:\",df3.shape) #reiew dataset shape\n",
    "print(\"\\nThere are missing values under columns:\\n\",df3.isnull().sum()) #check missing values\n",
    "df3.head(3) #view updates values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### OPERATION 5 ####\n",
    "\n",
    "# DROP words & authors less than 3 char, and more than 30 for authors \n",
    "\n",
    "df3['length'] = df3.words.str.len() #count words length\n",
    "df3['length2'] = df3.author.str.len() #count authors length\n",
    "df4 = df3[df3.length > 4] #drop words with less than 3 char..and incorrect author name values\n",
    "df4 = df4[df4.length2 > 4] #drop author with less than 3 char..and incorrect author name values\n",
    "df4 = df4[df4.length2 < 30] #drop author with more than 30 char..and incorrect author name values\n",
    "df4=df4.drop(columns=['length']) #drop - #count words length\n",
    "df4=df4.drop(columns=['length2']) # drop - #count authors length\n",
    "\n",
    "\n",
    "# COUNT word for each author\n",
    "\n",
    "df5=df4.groupby(['author', 'words']).size().reset_index(name='counts') #find count for words\n",
    "print(\"Dataset shape after removing words less than 3 words' charachters and dropping incorrect authros name values and adding words counts column:\",df5.shape) #review\n",
    "print(df5.head(3)) #view updated dataframe\n",
    "\n",
    "print(\"\\nNumber of authors is\",df5['author'].count()) #count authors\n",
    "print(\"Number of unqiue authors is\",df5['author'].nunique()) #count unqiue authors\n",
    "\n",
    "print(\"\\nNumber of words is\",df5['words'].count()) #count words\n",
    "print(\"Number of unqiue words is\",df5['words'].nunique()) #count unqiue words\n",
    "\n",
    "print(\"\\nNumber of min words count is\",df5['counts'].min()) #count words - MIN\n",
    "print(\"Number of max words count is\",df5['counts'].max()) #count words - MAX\n",
    "print(\"Mean words count is\",df5['counts'].mean()) #count words - MEAN\n",
    "\n",
    "\n",
    "# DROP words repeated less than 3 times\n",
    "df5 = df5[df5.counts > 2]\n",
    "\n",
    "print(\"\\nAFTER DROPPING WORDS REPEATED LESS THAN 3 TIMES ONLY - Number of min words count is\",df5['counts'].min()) #count words - MIN\n",
    "print(\"AFTER DROPPING WORDS REPEATED LESS THAN 3 TIMES ONLY - Number of max words count is\",df5['counts'].max()) #count words - MAX\n",
    "print(\"AFTER DROPPING WORDS REPEATED LESS THAN 3 TIMES ONLY - Mean words count is\",df5['counts'].mean()) #count words - MEAN\n",
    "\n",
    "print(\"\\nNEW - Number of authors is\",df5['author'].count()) #count authors\n",
    "print(\"NEW - Number of unqiue authors is\",df5['author'].nunique()) #count unqiue authors\n",
    "\n",
    "print(\"\\nNEW - Number of words is\",df5['words'].count()) #count words\n",
    "print(\"NEW - Number of unqiue words is\",df5['words'].nunique()) #count unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(df):\n",
    "### OPERATION 6 ####\n",
    "\n",
    "    #FIND top 5 words for each author\n",
    "    topk=5\n",
    "    df = df.groupby(['author']).apply(lambda x: x.nlargest(topk,['counts'])).reset_index(drop=True)\n",
    "    #print(\"Dataset shape selecting top 5 words for each author:\",df6.shape) #reiew dataset shape\n",
    "    #print(\"\\nNumber of authors is\",df6['author'].count()) #count authors\n",
    "    #print(\"Number of unqiue authors is\",df6['author'].nunique()) #count unqiue authors\n",
    "    #print(\"\\nNumber of words is\",df6['words'].count()) #count words\n",
    "    #print(\"Number of unqiue words is\",df6['words'].nunique()) #count unqiue words\n",
    "    #print(\"\\nNumber of min words count is\",df6['counts'].min()) #counts summary - MIN\n",
    "    #print(\"Number of max words count is\",df6['counts'].max()) #counts summary - MAX \n",
    "    #print(\"Mean words count is\",df6['counts'].mean()) #counts summary - MEAN\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#RUN PARALLELIZED PROCESS - OPERATION 6 ONLY\n",
    "#selecting top 5 words for each author using multiprocess function\n",
    "df = parallelize_dataframe(df5, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TOTAL TIME OF 6 OPERATIONS ####\n",
    "#end time calculating \n",
    "end = timer()\n",
    "print(\"Time taken:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zahra Shuaib ##\n",
    "## zahrashuaib@gmail.com ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
